---
title: "Coweeta Time Varying Breakdown"
output: html_document
date: "2025-02-20"
---





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = here::here())
```
# Packages and Data
```{r load packages, echo = FALSE}
library(tidyverse)
library(here)
library(brms)
library(tidybayes)
library(parallel)
library(litterfitter)
library(ggsci)
library(doSNOW)
library(foreach)
source(here("litterfitter_prediction_functions.R"))

```

# Load Data
```{r}
coweeta_df <- read_csv(here("wholestream_breakdown_calculations_timeKexploration.csv"))|>
  mutate(mass_prop = pct.mass.remain/100, .before = sample.id)|>
  filter(mass_prop >0)|> #filters out two NAs, one with negative mass remaining
  group_by(year, stream, block, mesh.type, species)|>
  mutate(cohort_id = as_factor(cur_group_id()), .before = sample.id)|>
  group_by(year, stream, block, species)|>
  mutate(pair_id = as_factor(cur_group_id()), .before = sample.id)|>
  group_by(year, stream, block, mesh.type, species, date.deploy, date.collected)|>
  mutate(sample_rep = row_number(), .before = sample.id)|>
  rename(afdm_final = "final.afdm.g")

length(unique(coweeta_df$cohort_id))


```
This dataset has `r length(unique(coweeta_df$cohort_id))` time series. 

# Visualization

```{r}
coweeta_df|>
  filter(species == "ACER")|>
  ggplot(aes(x = days.incubated, y = mass_prop, color = mesh.type, shape = as_factor(block), group = cohort_id))+
  geom_point()+
  scale_color_jco()+
  facet_wrap(year~stream)+
  scale_y_continuous(breaks = c(0,0.2,0.4,0.6,0.8,1))+
  theme_bw()

coweeta_df|>
  filter(species == "RHODO")|>
  ggplot(aes(x = days.incubated, y = mass_prop, color = mesh.type, shape = as_factor(block), group = cohort_id))+
  geom_point()+
  scale_color_jco()+
  facet_wrap(year~stream)+
  scale_y_continuous(breaks = c(0,0.2,0.4,0.6,0.8,1), limits = c(0,1))+
  theme_bw()

blah <- coweeta_df|>
  filter(species == "RHODO", cohort_id == 106)|>
  select(mass_prop, days.incubated, mesh.type, block, cohort_id)


```


# Fit models
This employs the use of the litterfitter package which optimizes model parameters for fitting non-linear analytical decomposition functions to mass loss data. 

## litterfitter approach
```{r full series fits}

if(file.exists(here("coweeta_litter_fits.rds"))){
  
  litter_fits <- readRDS(here("coweeta_litter_fits.rds"))

}else{

#set up data
nested_data <- coweeta_df|>
    select(cohort_id, days.incubated, mass_prop)|>
    #filter(cohort_id %in% seq(1:2))|> #for testing
    group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
    nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data)
cohort_names <- nested_data|>pull(cohort_id)
n_iters <- 999

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(14) 
registerDoSNOW(cl)


# Loop
litter_fits <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts, 
        .packages = c("tidyverse", "litterfitter", "future"), .inorder = TRUE#, .final = function(x) setNames(x, cohort_names)
        ) %dopar% {
          
          # filter to cohort data and unnest into normal data frame
      df <- nested_data|>filter(batch_number == i)|>unnest()
        
       cohort_id <- df|>first()|>pull(cohort_id)
          #negative exponential fit
          negexp_fit <- fit_litter(time = df$days.incubated,  
                                              mass.remaining = df$mass_prop,
                                              upper = c(2), 
                                              lower = c(1e-08),
                                              model = "neg.exp",
                                              iters = n_iters/10) # need far fewer iterations for only one parameter
          
          #weibull fit
          weibull_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              lower = c(0.1,0.001),#beta, then alpha 
                                              upper = c(25000,100),#beta, then alpha 
                                              model = "weibull",
                                              iters = n_iters)
        
          #discrete parallel fit
          discpar_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(1, 50, 50), #A, K1, K2 
                                              lower = c(0, 1e-12, 1e-17), #A, K1, K2
                                              model = "discrete.parallel",
                                              iters = n_iters)
        
          #discrete series fit
           discser_fit <- fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(1, 50, 2), #R, K1, K2
                                              lower = c(0, 1e-04, 1e-20), #R, K1, K2
                                              model = "discrete.series",
                                              iters = n_iters)
           
           # continuous quality/continuous exponential fit
            contqual_fit = fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(10^6, 50), #b,a
                                              lower = c(1e-04, 0.01), #b,a
                                              model = "cont.quality",
                                              iters = n_iters)
            
            # Not actually sure what the name of this is, but estimates a lower mass limit at which decomposition ceases
            negexplim_fit = fit_litter(time = df$days.incubated, 
                                              mass.remaining = df$mass_prop,
                                              upper = c(10, 50, 1),
                                              lower = c(1e-06, 1e-02, 1e-20),
                                              model = "neg.exp.limit",
                                              iters = n_iters)
          
            # Package up results for export
            tibble(cohort_id = cohort_id,
                   negexp_fit = list(negexp_fit), 
                    weibull_fit = list(weibull_fit),
                    discpar_fit = list(discpar_fit),
                    discser_fit = list(discser_fit),
                    contqual_fit = list(contqual_fit),
                    negexplim_fit = list(negexplim_fit))
            
   
        }     
stopCluster(cl)

# save to file
saveRDS(litter_fits, file = here("coweeta_litter_fits.rds")) 

# Loops took about 7 minutes with 14 cores
}

```



```{r Post leaching fits}


if(file.exists(here("coweeta_litter_truncfits.rds"))){
  
  litter_fits_pl <- readRDS(here("coweeta_litter_truncfits.rds"))

}else{

#set up data
nested_data <- coweeta_df|>
    filter(!cohort_id %in% c(102, 124))|> #Rhodo series with apparently high microbial growth - postleach values very high|>
    select(cohort_id, days.incubated, mass_prop)|>
    #filter(cohort_id %in% seq(1:2))|> #for testing
    group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
  filter(days.incubated >0)|> #series start at a week
  arrange(days.incubated)|>
  mutate(mean_initial_postleach = mean(mass_prop[days.incubated == min(days.incubated)]), 
         mass_prop_postleach = mass_prop/mean(mean_initial_postleach), 
         days.incubated_postleach = days.incubated - min(days.incubated))|>
  filter(days.incubated_postleach>0)|>
  nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data)
cohort_names <- nested_data|>pull(cohort_id)
n_iters <- 999

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(14) 
registerDoSNOW(cl)


# Loop
litter_fits_pl <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts, 
        .packages = c("tidyverse", "litterfitter", "future"), .inorder = TRUE#, .final = function(x) setNames(x, cohort_names)
        ) %dopar% {
          
          # filter to cohort data and unnest into normal data frame
      df <- nested_data|>filter(batch_number == i)|>unnest()
        
       cohort_id <- df|>first()|>pull(cohort_id)
          #negative exponential fit
          negexp_fit <- fit_litter(time = df$days.incubated_postleach,  
                                              mass.remaining = df$mass_prop,
                                              upper = c(2), 
                                              lower = c(1e-08),
                                              model = "neg.exp",
                                              iters = n_iters/10) # need far fewer iterations for only one parameter
          
          #weibull fit
          weibull_fit <- fit_litter(time = df$days.incubated_postleach, 
                                              mass.remaining = df$mass_prop_postleach,
                                              lower = c(0.1,0.001),#beta, then alpha 
                                              upper = c(25000,100),#beta, then alpha 
                                              model = "weibull",
                                              iters = n_iters)
        
          #discrete parallel fit
          discpar_fit <- fit_litter(time = df$days.incubated_postleach, 
                                              mass.remaining = df$mass_prop_postleach,
                                              upper = c(1, 50, 50), #A, K1, K2 
                                              lower = c(0, 1e-12, 1e-17), #A, K1, K2
                                              model = "discrete.parallel",
                                              iters = n_iters)
        
          #discrete series fit
           discser_fit <- fit_litter(time = df$days.incubated_postleach, 
                                              mass.remaining = df$mass_prop_postleach,
                                              upper = c(1, 50, 2), #R, K1, K2
                                              lower = c(0, 1e-04, 1e-20), #R, K1, K2
                                              model = "discrete.series",
                                              iters = n_iters)
           
           # continuous quality/continuous exponential fit
            contqual_fit = fit_litter(time = df$days.incubated_postleach, 
                                              mass.remaining = df$mass_prop_postleach,
                                              upper = c(10^6, 50), #b,a
                                              lower = c(1e-04, 0.01), #b,a
                                              model = "cont.quality",
                                              iters = n_iters)
            
            # Not actually sure what the name of this is, but estimates a lower mass limit at which decomposition ceases
            negexplim_fit = fit_litter(time = df$days.incubated_postleach, 
                                              mass.remaining = df$mass_prop_postleach,
                                              upper = c(10, 50, 1),
                                              lower = c(1e-06, 1e-02, 1e-20),
                                              model = "neg.exp.limit",
                                              iters = n_iters)
          
            # Package up results for export
            tibble(cohort_id = cohort_id,
                   negexp_fit = list(negexp_fit), 
                    weibull_fit = list(weibull_fit),
                    discpar_fit = list(discpar_fit),
                    discser_fit = list(discser_fit),
                    contqual_fit = list(contqual_fit),
                    negexplim_fit = list(negexplim_fit))
            
   
        }     
stopCluster(cl)

# save to file
saveRDS(litter_fits_pl, file = here("coweeta_litter_truncfits.rds")) 

# Loops took about 7 minutes with 14 cores
}

```

```{r extract and tidy model fits}
tidy_fits <- 
  litter_fits|>
  mutate(negexp_converge_code = map_dbl(negexp_fit, ~pluck(., "optimFit", "convergence")),
         negexp_k = map_dbl(negexp_fit, ~pluck(., "optimFit", "par", 1)),
         negexp_AICc = map_dbl(negexp_fit, ~pluck(., "fitAICc")),
         negexp_logLik = map_dbl(negexp_fit, ~pluck(., "logLik")),
         negexp_BIC = map_dbl(negexp_fit, ~pluck(., "fitBIC")),
         negexp_pred = map(negexp_fit, ~pluck(., "predicted"))
  )|>
  mutate(weibull_converge_code = map_dbl(weibull_fit, ~pluck(., "optimFit", "convergence")),
         weibull_beta = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 1)),
         weibull_alpha = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 2)),
         weibull_AICc = map_dbl(weibull_fit, ~pluck(., "fitAICc")),
         weibull_logLik = map_dbl(weibull_fit, ~pluck(., "logLik")),
         weibull_BIC = map_dbl(weibull_fit, ~pluck(., "fitBIC")),
         weibull_pred = map(weibull_fit, ~pluck(., "predicted"))
         )|>
  mutate(discpar_converge_code = map_dbl(discpar_fit, ~pluck(., "optimFit", "convergence")),
         discpar_a = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 1)),
         discpar_k1 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 2)),
         discpar_k2 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 3)),
         discpar_AICc = map_dbl(discpar_fit, ~pluck(., "fitAICc")),
         discpar_logLik = map_dbl(discpar_fit, ~pluck(., "logLik")),
         discpar_BIC = map_dbl(discpar_fit, ~pluck(., "fitBIC")),
         discpar_pred = map(discpar_fit, ~pluck(., "predicted"))
         )|>
  mutate(discser_converge_code = map_dbl(discser_fit, ~pluck(., "optimFit", "convergence")),
         discser_r = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 1)),
         discser_k1 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 2)),
         discser_k2 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 3)),
         discser_AICc = map_dbl(discser_fit, ~pluck(., "fitAICc")),
         discser_logLik = map_dbl(discser_fit, ~pluck(., "logLik")),
         discser_BIC = map_dbl(discser_fit, ~pluck(., "fitBIC")),
         discser_pred = map(discser_fit, ~pluck(., "predicted"))
         )|>
  mutate(contqual_converge_code = map_dbl(contqual_fit, ~pluck(., "optimFit", "convergence")),
         contqual_b = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 1)),
         contqual_a = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 2)),
         contqual_AICc = map_dbl(contqual_fit, ~pluck(., "fitAICc")),
         contqual_logLik = map_dbl(contqual_fit, ~pluck(., "logLik")),
         contqual_BIC = map_dbl(contqual_fit, ~pluck(., "fitBIC")),
         contqual_pred = map(contqual_fit, ~pluck(., "predicted"))
         )|>
  mutate(negexplim_converge_code = map_dbl(negexplim_fit, ~pluck(., "optimFit", "convergence")),
         negexplim_k = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 1)),
         negexplim_a = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 2)),
         negexplim_b = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 3)),
         negexplim_AICc = map_dbl(negexplim_fit, ~pluck(., "fitAICc")),
         negexplim_logLik = map_dbl(negexplim_fit, ~pluck(., "logLik")),
         negexplim_BIC = map_dbl(negexplim_fit, ~pluck(., "fitBIC")),
         negexplim_pred = map(negexplim_fit, ~pluck(., "predicted"))
  )|>
  select(-c(negexp_fit, weibull_fit, discser_fit, discpar_fit, contqual_fit, negexplim_fit))|>
  mutate(cohort_id = as_factor(cohort_id))|>
  left_join(coweeta_df|>
    select(cohort_id, mass_prop, block, species, stream, year, mesh.type, mean.temp)|>
   group_by(cohort_id)|>
    slice_head(n =1))

tidy_fits_pl <- 
  litter_fits_pl|>
  mutate(negexp_converge_code = map_dbl(negexp_fit, ~pluck(., "optimFit", "convergence")),
         negexp_k = map_dbl(negexp_fit, ~pluck(., "optimFit", "par", 1)),
         negexp_AICc = map_dbl(negexp_fit, ~pluck(., "fitAICc")),
         negexp_logLik = map_dbl(negexp_fit, ~pluck(., "logLik")),
         negexp_BIC = map_dbl(negexp_fit, ~pluck(., "fitBIC")),
         negexp_pred = map(negexp_fit, ~pluck(., "predicted"))
  )|>
  mutate(weibull_converge_code = map_dbl(weibull_fit, ~pluck(., "optimFit", "convergence")),
         weibull_beta = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 1)),
         weibull_alpha = map_dbl(weibull_fit, ~pluck(., "optimFit", "par", 2)),
         weibull_AICc = map_dbl(weibull_fit, ~pluck(., "fitAICc")),
         weibull_logLik = map_dbl(weibull_fit, ~pluck(., "logLik")),
         weibull_BIC = map_dbl(weibull_fit, ~pluck(., "fitBIC")),
         weibull_pred = map(weibull_fit, ~pluck(., "predicted"))
         )|>
  mutate(discpar_converge_code = map_dbl(discpar_fit, ~pluck(., "optimFit", "convergence")),
         discpar_a = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 1)),
         discpar_k1 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 2)),
         discpar_k2 = map_dbl(discpar_fit, ~pluck(., "optimFit", "par", 3)),
         discpar_AICc = map_dbl(discpar_fit, ~pluck(., "fitAICc")),
         discpar_logLik = map_dbl(discpar_fit, ~pluck(., "logLik")),
         discpar_BIC = map_dbl(discpar_fit, ~pluck(., "fitBIC")),
         discpar_pred = map(discpar_fit, ~pluck(., "predicted"))
         )|>
  mutate(discser_converge_code = map_dbl(discser_fit, ~pluck(., "optimFit", "convergence")),
         discser_r = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 1)),
         discser_k1 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 2)),
         discser_k2 = map_dbl(discser_fit, ~pluck(., "optimFit", "par", 3)),
         discser_AICc = map_dbl(discser_fit, ~pluck(., "fitAICc")),
         discser_logLik = map_dbl(discser_fit, ~pluck(., "logLik")),
         discser_BIC = map_dbl(discser_fit, ~pluck(., "fitBIC")),
         discser_pred = map(discser_fit, ~pluck(., "predicted"))
         )|>
  mutate(contqual_converge_code = map_dbl(contqual_fit, ~pluck(., "optimFit", "convergence")),
         contqual_b = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 1)),
         contqual_a = map_dbl(contqual_fit, ~pluck(., "optimFit", "par", 2)),
         contqual_AICc = map_dbl(contqual_fit, ~pluck(., "fitAICc")),
         contqual_logLik = map_dbl(contqual_fit, ~pluck(., "logLik")),
         contqual_BIC = map_dbl(contqual_fit, ~pluck(., "fitBIC")),
         contqual_pred = map(contqual_fit, ~pluck(., "predicted"))
         )|>
  mutate(negexplim_converge_code = map_dbl(negexplim_fit, ~pluck(., "optimFit", "convergence")),
         negexplim_k = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 1)),
         negexplim_a = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 2)),
         negexplim_b = map_dbl(negexplim_fit, ~pluck(., "optimFit", "par", 3)),
         negexplim_AICc = map_dbl(negexplim_fit, ~pluck(., "fitAICc")),
         negexplim_logLik = map_dbl(negexplim_fit, ~pluck(., "logLik")),
         negexplim_BIC = map_dbl(negexplim_fit, ~pluck(., "fitBIC")),
         negexplim_pred = map(negexplim_fit, ~pluck(., "predicted"))
  )|>
  select(-c(negexp_fit, weibull_fit, discser_fit, discpar_fit, contqual_fit, negexplim_fit))|>
  mutate(cohort_id = as_factor(cohort_id))|>
  left_join(coweeta_df|>
    select(cohort_id, mass_prop, block, species, stream, year, mesh.type, mean.temp)|>
   group_by(cohort_id)|>
    slice_head(n =1))

```


```{r}
alpha_by_type <- tidy_fits|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()+
  facet_wrap(mesh.type~species, scales = "free")

alpha_by_type2 <- tidy_fits|>filter(species != "WOOD")|>
  ggplot(aes(x = weibull_alpha))+geom_density()+
  facet_wrap(mesh.type~species, scales = "free_y")

alpha_by_type3 <- tidy_fits|>filter(species != "WOOD")|>
  ggplot(aes(x = weibull_alpha, fill = mesh.type))+geom_density()+
  facet_wrap(species~., scales = "free_y")+
  scale_fill_jco(alpha = 0.3)
  
  
ggsave(alpha_by_type, filename = "Figures/coweeta_alpha-by-type.jpeg", height = 8, width = 8)

tidy_fits|>
  filter(weibull_alpha <3)|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()

```

```{r postleach alpha viz}
alpha_by_type_postleach <- tidy_fits_pl|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()+
  facet_wrap(mesh.type~species, scales = "free")

alpha_by_type2_postleach <- tidy_fits_pl|>filter(species != "WOOD")|>
  ggplot(aes(x = weibull_alpha))+geom_density()+
  facet_wrap(mesh.type~species, scales = "free_y")

alpha_by_type3_postleach <- tidy_fits_pl|>filter(species != "WOOD")|>
  ggplot(aes(x = weibull_alpha, fill = mesh.type))+geom_density()+
  facet_wrap(species~., scales = "free_y")+
  scale_fill_jco(alpha = 0.3)
  
  
ggsave(alpha_by_type_postleach, filename = "Figures/coweeta_alpha-by-type_postleach.jpeg", height = 8, width = 8)

tidy_fits_pl|>
  filter(weibull_alpha <3)|>
  ggplot(aes(x = weibull_alpha))+geom_histogram()

```

```{r parsimony comparisons}

mod_gof <- tidy_fits|>
  #reshape for usability
  select(cohort_id, contains(c("AIC", "AICc", "BIC", "logLik")))|>
  pivot_longer(cols = -c(cohort_id), names_to = c("model", "gof_metric"), names_sep = "_", values_to = "value")|>
  filter(model %in% c("negexp","discpar","weibull"))|>
  group_by(cohort_id, gof_metric)|>
  mutate(best_in_cohort = case_when(gof_metric == "logLik" & value == max(value) ~ 1, #select highest when logLik
                                    gof_metric != "logLik" & value == min(value) ~ 1, # select lowest when AIC/BIC
                                    .default = 0))|>
    mutate(model_type = ifelse(model != "negexp", "non-constant", "negexp"))

mod_gof_pl <- tidy_fits_pl|>
  #reshape for usability
  select(cohort_id, contains(c("AIC", "AICc", "BIC", "logLik")))|>
  pivot_longer(cols = -c(cohort_id), names_to = c("model", "gof_metric"), names_sep = "_", values_to = "value")|>
  filter(model %in% c("negexp","discpar","weibull"))|>
  group_by(cohort_id, gof_metric)|>
  mutate(best_in_cohort = case_when(gof_metric == "logLik" & value == max(value) ~ 1, #select highest when logLik
                                    gof_metric != "logLik" & value == min(value) ~ 1, # select lowest when AIC/BIC
                                    .default = 0))|>
    mutate(model_type = ifelse(model != "negexp", "non-constant", "negexp"))

```

```{r}

fit_comparisons <- mod_gof|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model)|>
  count()|>
  ungroup()|>group_by(gof_metric)|>mutate(per = n/sum(n))

fit_comparisons_type <- mod_gof|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model_type)|>
  count()|>
  ungroup()




n_time_series <- length(unique(tidy_fits$cohort_id))

fit_counts.p <- fit_comparisons|>
  filter(gof_metric != "logLik")|>
  mutate(model = fct_relevel(model, "weibull", "negexp", after = Inf),
         model = fct_recode(model,Weibull = "weibull", NegExp = "negexp", `Continuous Quality` = "contqual", `Discrete Parallel` = "discpar", 
                            `Discrete Series` = "discser"))|>
  ggplot(aes(x = gof_metric, y = n, fill = model), alpha =  1)+
  geom_bar(stat = "identity")+
  scale_fill_jco()+
  scale_y_continuous(breaks = c(0,50,100, n_time_series))+
  guides(fill = guide_legend("Model"))+
  xlab("Model Comparison Metric")+
  ylab("Count")+
  theme_classic()+
  theme(axis.title = element_text(face = "bold", size = 16), axis.text = element_text(size = 14, color = "black"),
        legend.title = element_text(size = 14), legend.text = element_text(size = 12))

fit_counts.p

ggsave(fit_counts.p, filename = "Figures/coweeta_parsimony-best-model-counts.jpeg", height = 8, width = 8)

```

```{r}

fit_comparisons_pl <- mod_gof_pl|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model)|>
  count()|>
  ungroup()|>group_by(gof_metric)|>mutate(per = n/sum(n))

fit_comparisons_type_pl <- mod_gof_pl|>
  filter(best_in_cohort==1)|>
  group_by(gof_metric, model_type)|>
  count()|>
  ungroup()


n_time_series <- length(unique(tidy_fits_pl$cohort_id))

fit_counts_pl.p <- fit_comparisons_pl|>
  filter(gof_metric != "logLik")|>
  mutate(model = fct_relevel(model, "weibull", "negexp", after = Inf),
         model = fct_recode(model,Weibull = "weibull", NegExp = "negexp", `Continuous Quality` = "contqual", `Discrete Parallel` = "discpar", 
                            `Discrete Series` = "discser"))|>
  ggplot(aes(x = gof_metric, y = n, fill = model), alpha =  1)+
  geom_bar(stat = "identity")+
  scale_fill_jco()+
  scale_y_continuous(breaks = c(0,50,100, n_time_series))+
  guides(fill = guide_legend("Model"))+
  xlab("Model Comparison Metric")+
  ylab("Count")+
  theme_classic()+
  theme(axis.title = element_text(face = "bold", size = 16), axis.text = element_text(size = 14, color = "black"),
        legend.title = element_text(size = 14), legend.text = element_text(size = 12))+
  ggtitle("Postleach model comparison")

fit_counts_pl.p

ggsave(fit_counts_pl.p, filename = "Figures/coweeta_parsimony-best-model-counts_postleach.jpeg", height = 8, width = 8)

```

```{r plot weibull predictions full series}
newpreds_df <- tidy_fits|>
  rowwise()|>
  mutate(negexp_newpreds = list(predict_negexp(time = newtimes, k = negexp_k)),
         discser_newpreds = list(predict_discser(time = newtimes, r = discser_r, k1 = discser_k1, k2 = discser_k2)),
         discpar_newpreds = list(predict_discpar(time = newtimes, a = discpar_a, k1 = discpar_k1, k2 = discpar_k2)),
         contqual_newpreds = list(predict_contqual(time = newtimes, a = contqual_a, b = contqual_b)),
         weibull_newpreds = list(predict_weibull(time = newtimes, beta = weibull_beta, alpha = weibull_alpha)),
         )|>
  select(cohort_id, ends_with("newpreds"))|>
  mutate(days.incubated = list(newtimes))|>
  group_by(cohort_id)|>
  unnest(cols = c(days.incubated, ends_with("newpreds")))|>
  pivot_longer(cols = ends_with("newpreds"), values_to = ".pred", names_to = "model")|>
  mutate(model = str_remove(model, "_newpreds"))

preds_all_weibull <- coweeta_df|>select(cohort_id,days.incubated)|>
  left_join(newpreds_df|>filter(model == "weibull"))|>
  left_join(tidy_fits|>select(cohort_id, weibull_alpha))
  

weibull_preds_df<-coweeta_df|>
  select(cohort_id, days.incubated, mass_prop)|>
  group_by(cohort_id, stream, block, mesh.type, species, year)|>
  group_modify(~add_row(., days.incubated = 0, mass_prop = 0))|>
  complete(days.incubated = full_seq(days.incubated, 1))|>
  left_join(newpreds_df|>filter(model == "weibull"))|>left_join(tidy_fits|>select(weibull_alpha,cohort_id))|>
  left_join(coweeta_df|>group_by(cohort_id, stream, block, mesh.type, species, year)|>
              summarize(mean.temp = mean(mean.temp)))


weibulls.p <- ggplot(data = weibull_preds_df|>select(-model)|>filter(weibull_alpha < 30))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = weibull_alpha), alpha = 0.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull$days.incubated)))+
  #scale_color_jco()+
  scale_color_gradient2(trans = "log", low = "red", mid = "grey", high = "black", breaks = c(0.1,0.3,1,3))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Weibull~alpha)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), 
        legend.position = c(0.9,0.78), legend.background = element_blank())
weibulls.p

weibulls.p + scale_y_log10()

ggsave(weibulls.p, filename = "Figures/coweeta_weibull-spaghetti.jpeg", height = 8, width = 8)

weibulls.p2 <- ggplot(data = weibull_preds_df|>select(-model)|>filter(weibull_alpha < 30, species != "WOOD"))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = weibull_alpha), alpha = 0.5)+
  #scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull$days.incubated)))+
  #scale_color_jco()+
  scale_color_gradient2(trans = "log", low = "red", mid = "grey", high = "black", breaks = c(0.1,0.3,1,3))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Weibull~alpha)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), legend.background = element_blank())+
  facet_wrap(species~mesh.type, scales = "free_y")
weibulls.p2

weibullst.p2 <- ggplot(data = weibull_preds_df|>select(-model)|>filter(weibull_alpha < 20, species != "WOOD"))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = mean.temp), alpha = 0.5)+
  #scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull$days.incubated)))+
  #scale_color_jco()+
  #scale_color_gradient2(low = "red", mid = "grey", high = "black", breaks = c(8,10,12))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Temperature)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), legend.background = element_blank())+
  facet_wrap(species~mesh.type, scales = "free_y")
weibullst.p2

```

```{r plot weibull predictions truncated series}
newpreds_df_pl <- tidy_fits_pl|>
  rowwise()|>
  mutate(negexp_newpreds = list(predict_negexp(time = newtimes, k = negexp_k)),
         discser_newpreds = list(predict_discser(time = newtimes, r = discser_r, k1 = discser_k1, k2 = discser_k2)),
         discpar_newpreds = list(predict_discpar(time = newtimes, a = discpar_a, k1 = discpar_k1, k2 = discpar_k2)),
         contqual_newpreds = list(predict_contqual(time = newtimes, a = contqual_a, b = contqual_b)),
         weibull_newpreds = list(predict_weibull(time = newtimes, beta = weibull_beta, alpha = weibull_alpha)),
         )|>
  select(cohort_id, ends_with("newpreds"))|>
  mutate(days.incubated = list(newtimes))|>
  group_by(cohort_id)|>
  unnest(cols = c(days.incubated, ends_with("newpreds")))|>
  pivot_longer(cols = ends_with("newpreds"), values_to = ".pred", names_to = "model")|>
  mutate(model = str_remove(model, "_newpreds"))

preds_all_weibull_pl <- coweeta_df|>select(cohort_id,days.incubated)|>
  left_join(newpreds_df_pl|>filter(model == "weibull"))|>
  left_join(tidy_fits_pl|>select(cohort_id, weibull_alpha))
  

weibull_preds_df_pl<-coweeta_df|>
  select(cohort_id, days.incubated, mass_prop)|>
  group_by(cohort_id, stream, block, mesh.type, species, year)|>
  group_modify(~add_row(., days.incubated = 0, mass_prop = 0))|>
  complete(days.incubated = full_seq(days.incubated, 1))|>
  left_join(newpreds_df_pl|>filter(model == "weibull"))|>left_join(tidy_fits_pl|>select(weibull_alpha,cohort_id))|>
  left_join(coweeta_df|>group_by(cohort_id, stream, block, mesh.type, species, year)|>
              summarize(mean.temp = mean(mean.temp)))


weibulls_pl.p <- ggplot(data = weibull_preds_df_pl|>select(-model)|>filter(weibull_alpha < 30))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = weibull_alpha), alpha = 0.5)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull_pl$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull_pl$days.incubated)))+
  #scale_color_jco()+
  scale_color_gradient2(trans = "log", low = "red", mid = "grey", high = "black", breaks = c(0.1,0.3,1,3))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Weibull~alpha)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), 
        legend.position = c(0.9,0.78), legend.background = element_blank())
weibulls_pl.p

weibulls_pl.p + scale_y_log10()

ggsave(weibulls_pl.p, filename = "Figures/coweeta_weibull-spaghetti_postleach.jpeg", height = 8, width = 8)

weibulls_pl.p2 <- ggplot(data = weibull_preds_df_pl|>select(-model)|>filter(weibull_alpha < 30, species != "WOOD"))+
  geom_line(aes(x = days.incubated, y = .pred, group = cohort_id, color = mean.temp), alpha = 0.5)+
  #scale_y_continuous(limits = c(0, 1), breaks = c(0,0.25,0.5,0.75,1.0))+
  scale_x_continuous(limits = c(0,max(preds_all_weibull_pl$days.incubated)), breaks = c(0,50,100,150,200, max(preds_all_weibull_pl$days.incubated)))+
  #scale_color_jco()+
  #scale_color_gradient2(low = "red", mid = "grey", high = "black", breaks = c(8,10,12))+
  #scale_color_gradient2()+
  guides(color = guide_colorbar(title = expression(Temperature)))+
  xlab("Day")+
  ylab("Proportional Mass Remaining")+
  theme_classic()+
  theme(panel.background = element_rect(fill = "#F7F9F9"),
        legend.title = element_text(size = 20), legend.text = element_text(size = 16), legend.background = element_blank())+
  facet_wrap(species~mesh.type, scales = "free_y")
weibulls_pl.p2

```


```{r alpha temperature effect}

ggplot(tidy_fits|>filter(species != "WOOD", weibull_alpha < 20),
       aes(x = mean.temp, y = weibull_alpha, shape = year))+
  geom_point()+
  facet_wrap(species~mesh.type, scales = "free")+
  geom_smooth(method = "lm")+
  ggtitle("Weibull Alpha")

ggplot(tidy_fits_pl|>filter(species != "WOOD", weibull_alpha < 20),
       aes(x = mean.temp, y = weibull_alpha, shape = year))+
  geom_point()+
  facet_wrap(species~mesh.type, scales = "free")+
  geom_smooth(method = "lm")+
  ggtitle("Postleach Weibull Alpha")


ggplot(tidy_fits|>filter(species != "WOOD", weibull_alpha < 20),
       aes(x = mean.temp, y = weibull_beta, shape = year))+
  geom_point()+
  facet_wrap(species~mesh.type, scales = "free")+
  geom_smooth(method = "lm")+
  ggtitle("Weibull Beta")

ggplot(tidy_fits_pl|>filter(species != "WOOD", weibull_alpha < 20),
       aes(x = mean.temp, y = weibull_beta, shape = year, color = stream))+
  geom_point()+
  facet_wrap(species~mesh.type, scales = "free")+
  #geom_smooth(method = "lm")+
  ggtitle("Postleach Weibull Beta")

ggplot(tidy_fits|>filter(species != "WOOD", weibull_alpha < 20),
       aes(x = mean.temp, y = negexp_k, shape = year))+
  geom_point()+
  facet_wrap(species~mesh.type, scales = "free")+
  geom_smooth(method = "lm")+
  ggtitle("k")

ggplot(tidy_fits_pl|>filter(species != "WOOD", weibull_alpha < 20))+
  geom_point(aes(x = mean.temp, y = negexp_k, shape = year, color = stream))+
  facet_wrap(species~mesh.type, scales = "free")+
  geom_smooth(aes(x = mean.temp, y = negexp_k),method = "lm")+
  ggtitle("Preleach k")+
  scale_y_continuous(trans = "log")

```
Interestingly no pattern of temperature influencing the shape of the postleach trajectories. I wonder if temperature in the early stage is what matters most.



```{r bootstrap alpha}
tic()
#set up data
nested_data <- litter_fits|>
    select(cohort_id, weibull_fit)|>
    #filter(cohort_id %in% seq(1:4))|> #for testing
    group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
    nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data)
cohort_names <- nested_data|>pull(cohort_id)
n_iters <- 999

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(4) 
registerDoSNOW(cl)

# Loop
weibull_alpha_bs <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts, 
        .packages = c("tidyverse", "litterfitter"), .inorder = TRUE#, .final = function(x) setNames(x, cohort_names)
        ) %dopar% {
          
          # filter to cohort data and unnest into normal data frame
      df <- nested_data|>filter(batch_number == i)|>unnest()
  
       cohort_id <- df|>first()|>pull(cohort_id)
       weibull_fit <- df|>pluck("weibull_fit",1)
       

          
            weibull_bs <- litterfitter::bootstrap_parameters(x = weibull_fit, nboot = n_iters, lower = c(0.1,0.001), 
                                               upper = c(25000,100)) #beta, then alpha - same bounds used for main litter fitting exercise
            
           alpha <- weibull_bs[, 2] # [[,1] is beta, [,3] is the steady state
           beta <- weibull_bs[, 1]
  
          qfs <- quantile(alpha, probs = c(0.025, 0.975))
       
          
       tibble(cohort_id = cohort_id,
              weibull_alpha_dist = list(alpha),
              weibull_beta_dist = list(beta),
              weibull_alpha_025 = qfs[1],
              weibull_alpha_975 =  qfs[2])
   
        }     
stopCluster(cl)

weibull_alpha_bs2 <- weibull_alpha_bs|>
  rowwise()|>
  mutate(weibulla_overlap_1 = between(1, weibull_alpha_025,weibull_alpha_975))

toc()
# save to file
#weibull_alpha_bs2|>write_csv(file = here("data/derived_data/DetNut_weibullalpha_bootstraps.csv")) 

```


## Bayesian approach

```{r formula and priors}

negexpcohort_bf <- bf(mass_prop ~ exp(-k*days.incubated), 
                      k ~0+cohort_id, nl = TRUE)
negexpcohort_priors <- prior(lognormal(-6,2), nlpar = "k", lb = 0)

weibullcohort_bf <- bf(mass_prop ~ exp(-((days.incubated)/(beta))^(alpha)), 
                 beta ~ 0 + cohort_id, alpha ~ 0 + cohort_id, nl = TRUE)
weibullcohort_priors <- prior(gamma(1,0.2), nlpar = "alpha", lb = 0)+
  prior(lognormal(5.5,1), nlpar = "beta", lb = 0)

discsercohort_bf <- bf(mass_prop ~ (((1 - r)*k1*exp(-k2*days.incubated)) - ((k2 - k1*r)*exp(-k1*days.incubated)))/(k1 - k2),
                 r ~ 0 + cohort_id,  k1 ~ 0 + cohort_id, k2 ~ 0+cohort_id, nl = TRUE)

discsercohort_priors <- prior(beta(1,2.5), nlpar = "r", lb = 0, ub = 1)+
  prior(lognormal(-2.25,1.5), nlpar = "k1", lb = 0)+
  prior(lognormal(-4.5,2), nlpar = "k2", lb = 0)


```



```{r fit brm}
 
#fit in a few minutes
negexpcohort_fits_brm <- brm(formula = negexpcohort_bf, prior = negexpcohort_priors,
                 data = coweeta_df, 
                 family = "gaussian", 
                 iter = 5000, chains = 4, cores = 4, warmup = 2000,
                 control= list(adapt_delta = 0.99), 
                 file = here("data/brms_objects/negexpcohort_fits_brm"))


tictoc::tic()
weibullcohort_fits_brm <- brm(formula = weibullcohort_bf, prior = weibullcohort_priors,
                 data = coweeta_df, 
                 family = "gaussian", 
                 iter = 5000, chains = 4, cores = 4, warmup = 2000,
                 control= list(adapt_delta = 0.99), 
                 file = here("data/brms_objects/weibullcohort_fits_brm"))
tictoc::toc() # about an hour - only 1 divergent transition

weibullcohort_fits_brm<- readRDS(here("data/brms_objects/weibullcohort_fits_brm.Rds"))

blah <- fixef(weibullcohort_fits_brm)|>as.data.frame()|>rownames_to_column("coef")|>mutate(param = ifelse(str_detect(coef, "alpha"), "alpha", "beta"))|>filter(param == "alpha")

weibull_test <- brm(bf(formula = mass_prop ~ exp(-((days.incubated)/(beta))^(alpha)), 
                 beta + alpha ~ 1, nl = TRUE), prior = weibullcohort_priors,
                 data = coweeta_df|>filter(cohort_id == 123), 
                 family = "gaussian", 
                 iter = 5000, chains = 4, cores = 4, warmup = 2000,
                 control= list(adapt_delta = 0.99))




tictoc::tic()
discsercohort_fits <- brm(formula = discsercohort_bf, prior = discsercohort_priors,
                 data = coweeta_df|>filter(cohort_id %in% c(1,2)), 
                 family = "gaussian", 
                 iter = 5000, chains = 4, cores = 4, warmup = 2000,
                 control= list(adapt_delta = 0.99))
tictoc::toc()

marginaleffects::plot_predictions(negexpcohort_fits, condition = c("days.incubated", "cohort_id"), points = 0.7)

marginaleffects::plot_predictions(weibull_test, condition = c("days.incubated"), points = 0.7)

marginaleffects::plot_predictions(weibullcohort_fits, condition = c("days.incubated", "cohort_id"), points = 0.7)

marginaleffects::plot_predictions(discser_fits, condition = c("days.incubated", "cohort_id"), points = 0.7)

```

Maybe what you really need to do here is model the coarse/fine pairs as a mv model... 

```{r pull weibull draws}
weibull_draws_dat <- coweeta_df|>
  group_by(cohort_id, mesh.type)|>nest()|>
  mutate(#final_day = map(data, ~max(.x$days.incubated)), 
         time_seq = map(data, ~seq(0, max(.x$days.incubated), by = 1)))|>
  select(-data)|>unnest(cols = c(time_seq))

  weibull_draws <- tidybayes::add_epred_draws(object = weibullcohort_fits_brm, ndraws = 5)
  
#loop through time series to create draws objects. 


```


```{r}


#set up nested dataframe
coweeta_pairs <- coweeta_df|>ungroup()|>
  filter(species != "WOOD")|>
  select(pair_id, stream, block, species, year, mesh.type, days.incubated, mass_prop)|>
  group_by(pair_id)|>nest()|>ungroup()|>
  arrange(as.numeric(as.character(pair_id)))
  #slice_head(n = 2)

# Set up formulae
weibullpairs_bf <- bf(mass_prop ~ exp(-((days.incubated)/(beta))^(alpha)), 
                 beta ~ 0 + mesh.type, alpha ~ 0 + mesh.type, nl = TRUE, 
                 sigma ~ 0 + mesh.type)

path_pref <- "data/coweeta_pairs/weibull_fits/pair_id-"

# Loop and save fits
tic() # 90+ minutes
for(i in 1:length(coweeta_pairs$data)) {
  
  pair_id <- as.numeric(coweeta_pairs$pair_id[[i]])
  
  print(paste0("Fitting series pair_id ", pair_id, " (", i, " of ", length(coweeta_pairs$data), ")"))
  
  df <- coweeta_pairs$data[[i]]
  
  filename <- paste0(path_pref,pair_id)
  

  brm(weibullpairs_bf, 
                 data = df, 
                 prior = weibullcohort_priors,
                 iter = 5000, chains = 4, cores = 4, warmup = 2000,
                 control= list(adapt_delta = 0.99), refresh = 0,
    file = here(filename))
  
}

toc() 


```


```{r}

# Read files back in to do processing - functions:

#for(i in 1:length(coweeta_pairs$data)) {
#  assign(paste0("pair_", i, "_fit"),readRDS(here(paste0(path_pref, i, ".Rds"))), envir = #.GlobalEnv)
#}

new_df <- coweeta_pairs|>
  filter(pair_id %in% c(1,2,7))|>
  mutate(summarized_draws = map(pair_id, ~summarize_pair_draws(pair_i = .)))
         
summarize_pair_draws <- function(pair_i){
  
  fit <-  readRDS(here(paste0(path_pref, pair_i, ".Rds")))

coarse_days <- pluck(fit, "data")|>filter(mesh.type == "Coarse")|>pull(days.incubated)|>max()
fine_days <- pluck(fit, "data")|>filter(mesh.type == "Fine")|>pull(days.incubated)|>max()

pred_grid <- tibble(days.incubated = seq(0,coarse_days, by = 1),
                    mesh.type = rep("Coarse", length(days.incubated)))|>
  bind_rows(tibble(days.incubated = seq(0,fine_days, by = 1),
                    mesh.type = rep("Fine", length(days.incubated)))
  )

pred_draws <- add_epred_draws(fit, newdata = pred_grid, ndraws = 999)

summarized_draws <- pred_draws|>group_by(mesh.type, days.incubated)|>
  summarize(estimate = mean(.epred), 
            upper95 = quantile(.epred, 0.975), lower95 = quantile(.epred, 0.025),
            upper90 = quantile(.epred, 0.95), lower90 = quantile(.epred, 0.05),
            upper68 = quantile(.epred, 0.675), lower68 = quantile(.epred, 0.325))|>
  mutate(pair_id = as_factor(pair_i))|>
  left_join(coweeta_df|>group_by(pair_id)|>slice_head(n=1)|>
              select(mesh.type, pair_id, cohort_id, year, stream, block, species))


deriv_draws <- pred_draws|>ungroup()|>
  select(mesh.type, days.incubated, .draw, .epred)|>
  group_by(.draw, mesh.type)|>arrange(days.incubated)|>
  mutate(logmass = log(.epred),
         k_app = logmass - lag(logmass))

summarized_deriv_draws <- deriv_draws|>
  filter(days.incubated > 0)|>
  group_by(mesh.type, days.incubated)|>
  summarize(estimate = mean(k_app), 
            upper95 = quantile(k_app, 0.975), lower95 = quantile(k_app, 0.025),
            upper90 = quantile(k_app, 0.95), lower90 = quantile(k_app, 0.05),
            upper68 = quantile(k_app, 0.84), lower68 = quantile(k_app, 0.16),
            sd = sd(k_app))|>
  mutate(pair_id = as_factor(pair_i))|>
  left_join(summarized_draws|>select(estimate, mesh.type, days.incubated)|>
              rename(estimated_mass = "estimate"))|>relocate(estimated_mass, .after = days.incubated)

summarized_F <- deriv_draws|>ungroup()|>
  select(-c(logmass, .epred))|>
  pivot_wider(names_from = mesh.type, values_from = k_app)|>
  mutate(lambda_F = Coarse - Fine*exp(Coarse - Fine))|>
  group_by(days.incubated)|>
  filter(days.incubated >0, !is.na(lambda_F))|>
  summarize(estimate = mean(lambda_F), 
            upper95 = quantile(lambda_F, 0.975), lower95 = quantile(lambda_F, 0.025),
            upper90 = quantile(lambda_F, 0.95), lower90 = quantile(lambda_F, 0.05),
            upper68 = quantile(lambda_F, 0.84), lower68 = quantile(lambda_F, 0.16),
            sd = sd(lambda_F))|>
  mutate(pair_id = as_factor(pair_i))|>
  left_join(coweeta_df|>group_by(pair_id)|>slice_head(n=1)|>
              select(mesh.type, pair_id, cohort_id, year, stream, block, species))|>
  left_join(summarized_draws|>select(estimate, mesh.type, days.incubated)|>
              rename(estimated_mass = "estimate"))|>relocate(estimated_mass, .after = days.incubated)

list("summarized_draws" = summarized_draws,
     "summarized_deriv_draws" = summarized_deriv_draws,
     "summarized_F_draws" = summarized_F)
}

         
```

```{r plots}
new_df2 <- new_df|>
  left_join(coweeta_df|>ungroup()|>select(pair_id, stream, block, species, year)|>group_by(stream, block, species, year)|>slice_head(n= 1))


# plot fragmentation rates - start with mean estimates - two spaghetti plots, one for each species showing variation in fragmentation rates through

frags_df <- new_df2|>
  hoist(summarized_draws, "summarized_F_draws")

ggplot(data, aes(x = days.incu))

```


o	
-	Time-varying fragmentation rates – 2 stage approach
o	Basically just spaghetti plots but could also take similar approach to see if temperature sensitivity of fragmentation rates (overall) or time-varying
	Single fragmentation rate of Lecerf
	Average fragmentation rate from time-varying approach (w/ uncertainty)
	These values with leaching stage removed
# 2 stage analysis

Extract instantaneous slopes at ~ 10%, 30%, 50%, and 80% mass loss
## Temperature sensitivity 
- Single k (what does propagating uncertainty do to the analysis?)
- Postleach k
- k' stages


## Fragmentation rates
- Single fragmentation rate of Lecerf
-- Summarize by plots of F ~ temp, facet species
-- Propagate error through the calculation?
- Average fragmentation rate from time-varying approach
- Average fragmentation with leaching stage removed
- Continuous fragmentation rates


# Deprecated
```{r}

test_dat <- coweeta_pairs|>filter(pair_id == 7)|>slice_head(n = 1)
for(i in 1:length(test_dat$data)) {

  print(paste0("Summarizing pair ", i))
  
  fit <-  readRDS(here(paste0(path_pref, i, ".Rds")))
  
  
coarse_days <- pluck(fit, "data")|>filter(mesh.type == "Coarse")|>pull(days.incubated)|>max()
fine_days <- pluck(fit, "data")|>filter(mesh.type == "Fine")|>pull(days.incubated)|>max()

pred_grid <- tibble(days.incubated = seq(0,coarse_days, by = 1),
                    mesh.type = rep("Coarse", length(days.incubated)))|>
  bind_rows(tibble(days.incubated = seq(0,fine_days, by = 1),
                    mesh.type = rep("Fine", length(days.incubated)))
  )

pred_draws <- add_epred_draws(fit, newdata = pred_grid, ndraws = 999)

summarized_draws <- pred_draws|>group_by(mesh.type, days.incubated)|>
  summarize(estimate = mean(.epred), upper95 = quantile(.epred, 0.975), lower95 = quantile(.epred, 0.025))|>
  mutate(pair_id = as_factor(i))|>
  left_join(coweeta_df|>group_by(pair_id)|>slice_head(n=1)|>
              select(mesh.type, pair_id, cohort_id, year, stream, block, species))


deriv_draws <- pred_draws|>ungroup()|>
  select(mesh.type, days.incubated, .draw, .epred)|>
  group_by(.draw, mesh.type)|>arrange(days.incubated)|>
  mutate(logmass = log(.epred),
         k_app = logmass - lag(logmass))

deriv_draws_for_csv <- deriv_draws|>
  mutate(pair_id = as_factor(i))|>
  left_join(coweeta_df|>group_by(pair_id)|>slice_head(n=1)|>
              select(mesh.type, pair_id, cohort_id, year, stream, block, species))

summarized_F <- deriv_draws|>ungroup()|>
  select(-c(logmass, .epred))|>
  pivot_wider(names_from = mesh.type, values_from = k_app)|>
  mutate(lambda_F = Coarse - Fine*exp(Coarse - Fine))|>
  group_by(days.incubated)|>
  filter(days.incubated >0)|>
  summarize(estimate = mean(lambda_F), upper95 = quantile(lambda_F, 0.975), lower95 = quantile(lambda_F, 0.025))|>
  mutate(pair_id = as_factor(i))|>
  left_join(coweeta_df|>group_by(pair_id)|>slice_head(n=1)|>
              select(mesh.type, pair_id, cohort_id, year, stream, block, species))
}

summarized_draws|>write_csv(paste0(here("data/coweeta_pairs/weibull_summarized-epred/fit_"), i, ".csv"))

deriv_draws_for_csv|>write_csv(paste0(here("data/coweeta_pairs/weibull_deriv-epred-draws/fit_"), i, ".csv"))

summarized_F|>write_csv(paste0(here("data/coweeta_pairs/weibull_summarized-F/fit_"), i, ".csv"))

}

```


```{r}
#calculate epred draws on prediction grid
# Calculate fragmentation rates
# 

coarse_days <- df|>filter(mesh.type == "Coarse")|>pull(days.incubated)|>max()
fine_days <- df|>filter(mesh.type == "Fine")|>pull(days.incubated)|>max()

pred_grid <- tibble(days.incubated = seq(0,coarse_days, by = 1),
                    mesh.type = rep("Coarse", length(days.incubated)))|>
  bind_rows(tibble(days.incubated = seq(0,fine_days, by = 1),
                    mesh.type = rep("Fine", length(days.incubated)))
  )

# Sequence:
# - 1) Summarize expected value distributions at each point in the grid
# - 2) Calculate difference and summarize expected value difference at each point in the grid
# - 3a) calculate instantaneous k for coarse and fine at each point in the grid
# - 3b) Calculate fragmentation using instantaneous k distros

pred_draws <- add_epred_draws(pairs_fit, newdata = pred_grid, ndraws = 999)

summarized_draws <- pred_draws|>group_by(mesh.type, days.incubated)|>
  summarize(estimate = mean(.epred), upper95 = quantile(.epred, 0.975), lower95 = quantile(.epred, 0.025))

difference_draws <- test_draws|>ungroup()|>
  select(mesh.type, days.incubated, .draw, .epred)|>
  group_by(.draw, mesh.type)|>arrange(days.incubated)|>
  mutate(logmass = log(.epred),
         k_app = logmass - lag(logmass))

summarize_F <- difference_draws|>ungroup()|>
  select(-c(logmass, .epred))|>
  pivot_wider(names_from = mesh.type, values_from = k_app)|>
  mutate(lambda_F = Coarse - Fine*exp(Coarse - Fine))|>
  group_by(days.incubated)|>
  filter(days.incubated >0)|>
  summarize(estimate = mean(lambda_F), upper95 = quantile(lambda_F, 0.975), lower95 = quantile(lambda_F, 0.025))

ggplot(summarize_F|>filter(days.incubated > 7))+
  geom_line(aes(x = days.incubated, y = estimate))+
  geom_line(aes(x = days.incubated, y = upper95), linetype = "dashed")+
  geom_line(aes(x = days.incubated, y = lower95), linetype = "dashed")

```


```{r}


test_preds <- marginaleffects::predictions(pairs_fit, newdata = test_grid)

blah <- comparisons(pairs_fit, variables = "mesh.type", newdata = test_grid)|>filter(mesh.type == "Coarse")



# what you need here is instantaneous k, so a grid of ln(M) derivatives



ggplot(test_preds)+
  geom_line(aes(x = days.incubated, y = estimate, color = mesh.type))+
  geom_line(aes(x = days.incubated, y = conf.high, color = mesh.type), linetype = "dashed")+
  geom_line(aes(x = days.incubated, y = conf.low, color = mesh.type), linetype = "dashed")+
  geom_point(data = test, aes(x = days.incubated, y = mass_prop, color = mesh.type), alpha = 0.8, size = 2.5)+
  ggsci::scale_color_jco()+
  theme_bw()



ggplot(blah)+
  geom_line(aes(x = days.incubated, y = estimate))+
  geom_line(aes(x = days.incubated, y = conf.high), linetype = "dashed")+
  geom_line(aes(x = days.incubated, y = conf.low), linetype = "dashed")+
  ylab("Fine - Coarse")+
  theme_bw()


```






```{r test plot bs}
blah <- weibull_alpha_bs|>
  left_join(coweeta_df2|>
              group_by(cohort_id, species, block, stream, year, mesh.type)|>
              summarize(max_time = max(time)))|>
  slice_head(n = 1)|>
  unnest(cols = c(weibull_alpha_dist, weibull_beta_dist))|>
  rowwise()|>
  mutate(times = list(seq(0, max_time, by = 1)),
         preds = list(predict_weibull(time = times, alpha = weibull_alpha_dist, beta = weibull_beta_dist)))|>
  mutate(index = row_number())

ggplot(aes(x = unlist(times), y = unlist(preds), group = index))+
  geom_line()



```

```{r brms fits}

# Fit Bayesian litter fits

# Load data
coweeta_df2 <- coweeta_df|>
  filter(species != "WOOD")|>
  rename(mass = "mass_prop", time = "days.incubated")

# Set test data
series_ids <- unique(coweeta_df2$cohort_id)
series_ids <- series_ids[1:4] #testing


# Set priors and formulae

## K

k_bf <- bf(mass ~ exp(-k*time), k ~1, nl = TRUE)
k_priors <- prior(lognormal(-6,2), nlpar = "k", lb = 0)

## Weibull
weibull_bf <- bf(mass ~ exp(-(time/beta)^alpha), beta + alpha ~1, nl = TRUE)
weibull_priors <- prior(gamma(1,7), nlpar = "alpha", lb = 0)+
  prior(lognormal(5.5,1), nlpar = "beta", lb = 0)

## Discrete Parallel
discpar_bf <- bf(mass ~ a*exp(-k1 * time) + (1 - a) * exp(-k2 * time),
                 a + k1 + k2 ~ 1, nl = TRUE)


## Discrete Series



# Fitting loops
nested_data_brm <- coweeta_df2|>
  filter(cohort_id %in% series_ids)|>
  select(cohort_id, time, mass)|>
  #filter(cohort_id %in% seq(1:38))|> #for testing
  group_by(cohort_id)|> # needs group_by()|>nest() structure... nest_by() doesn't work!
  nest()|>
  ungroup()|>
  mutate(batch_number = row_number())

# Definitions for use in parallel loops
n_data_batches <- nrow(nested_data_brm)
cohort_names <- nested_data_brm|>pull(cohort_id)

pb <- txtProgressBar(max = n_data_batches, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

# Set parallel processing
cl <- makePSOCKcluster(4)
registerDoSNOW(cl)
tic()
# Loop
weibull_summaries <- foreach(i = 1:nrow(nested_data), .combine = 'rbind', .options.snow = opts,
                         .packages = c("tidyverse", "brms", "here", "tidybayes"), .inorder = FALSE
) %dopar% {

  # filter to cohort data and unnest into normal data frame
  df <- nested_data_brm|>filter(batch_number == i)|>unnest()
  cohort_id <- df|>first()|>pull(cohort_id)
  file_name <- paste0("cohort_", cohort_id)

  fit <- brm(weibull_bf,
             prior = weibull_priors,
             data = df,
             chains = 4, cores = 1, iter = 4000,
             warmup = 500, control= list(adapt_delta = 0.99),
             file = paste0("analysis/data/derived_data/brms_fits/weibull/",
                           file_name)
  )

  summarise_draws(fit)|>
    tibble(cohort_id = cohort_id,
           model_type = "Weibull")|>
      relocate(cohort_id, model_type)


}
stopCluster(cl)
toc()
# save model objects, summarize parameter HDIs, diagnostic values, to table
fit <- readRDS(here("analysis/data/derived_data/brms_fits/weibull/cohort_16.Rds"))
summary(fit)
summarise_draws(fit)
```

# Test hypotheses

- Coarse and fine mesh have similar early stage decomposition
- Fragmentation 
